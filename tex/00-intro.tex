\begin{framed}
\noindent
 \textbf{Note.} Here are some things to keep in mind as you plan your time for this assignment.
\begin{itemize}
   \item The total amount of pytorch code to write, and code complexity, of this assignment is lower than Assignment 4. 
         However, you're also given less guidance or scaffolding in how to write the code.
         \item  This assignment involves a pretraining step that takes approximately 2 hours to perform on Azure, and you'll have to do it twice.
\end{itemize}
\end{framed}
This assignment is an investigation into Transformer self-attention building blocks, and the effects of pretraining.
It covers mathematical properties of Transformers and self-attention through written questions.
Further, you'll get experience with practical system-building through repurposing an existing codebase.
The assignment is split into a coding part and an extra credit written (mathematical) part.
Here's a quick summary:
\begin{enumerate}
\item \textbf{Extending a research codebase:}
In this portion of the assignment, you'll get some experience and intuition for a cutting-edge research topic in NLP: teaching NLP models facts about the world through pretraining, and accessing that knowledge through finetuning.
You'll train a Transformer model to attempt to answer simple questions of the form ``Where was person [x] born?'' -- without providing any input text from which to draw the answer.
You'll find that models are able to learn some facts about where people were born through pretraining, and access that information during fine-tuning to answer the questions.

Then, you'll take a harder look at the system you built, and reason about the implications and concerns about relying on such implicit pretrained knowledge.

\item \textbf{Mathematical exploration:}  What kinds of operations can self-attention easily implement? Why should we use fancier things like multi-headed self-attention?
This section will use some mathematical investigations to illuminate a few of the motivations of self-attention and Transformer networks.

%\begin{enumerate}
%\item \textbf{Trying out the existing code (\mingpt):}
%It's frequently the case in research that you'd like to try out a few things with code that you know works, and is simple enough to make some small tweaks.
%A few quick experiments can save you a lot of research time!
%You'll start with (a slightly modified version of) \mingpt, a fully functioning implementation of the GPT Transformer-based language model.
%\item \textbf{Writing a self-supervision function:} You'll then write code to construct self-supervision examples -- when we explicitly corrupt part of an input sentence, and ask the model to reconstruct what was destroyed through noise.
%\item \textbf{Writing a data loader for our knowledge access task:} Next, you'll write some simple code to load examples of our knowledge-access task. We'll use the toy task of accessing where a person was born based on their Wikipedia entry.
%\item \textbf{Comparing pretrained vs non-pretrained models:} You'll next train two models on the knowledge access task: a pretrained model (using the task you constructed) and a non-pretrained model. You'll report their accuracies.
%\item \textbf{Making an \textit{additive attention} variant} Finally, you'll modify the Transformer code to change the type of attention used, and report pretrained and non-pretrained knowledge-access values on this model type.
%\end{enumerate}

%\item \textbf{Extending a research codebase:} In your future as an ML engineer extraordinaire, AI researcher extraordinaire, quantitative social scientist extraordinaire, tech-aware public policy consultant extraordinare, or otherwise, you'll probably want to take an existing codebase from a paper and (perhaps extensively) modify it to suit your purposes.
%In this question, we'll get practice through an ideal case of this.
%\begin{enumerate}
%\item \textbf{Trying out existing code, \mingpt:} Imagine this: you want to do a \textit{language modeling} task, and you hear that Transformers are all the rage.
%Being exceptionally studious, you decide to start with a codebase that clearly but minimally implements the functionality you need, and build everything else yourself.
%You start with \mingpt, frankly an excellent codebase containing a fully functioning implementation of the GPT Transformer-based language model.
%It has a nice Jupyter notebook interface; you'll fork the code and run it!
%\item 
%\item \textbf{Actually, I want encoder-decoder, not language modeling...} Uh oh, upon further thought, you realize that the task you're thinking of implmenenting requires you to generate some text conditioned not just on past tokens, but also on future tokens... to feed some part of the input to the model before starting to ask the model to generate something, which means you really want an encoder-decoder model.
%In this section, you'll extend the \mingpt codebase to implement a Transformer encoder-decoder model, like the recent T5 model.
%As an added bonus, our new model, instead of \mingpt, is called \ourmodel, with the adorable nickname ``Minty''.
%\item \textbf{I've got this great idea to save some time...}
%Some of the more radical members of your group have been whispering about how there's quadratic running time complexity in Transformers, and since you haven't \textit{yet} convinced Google that your project deserves all of their compute, you'd prefer something faster for the time being.
%You'll implement a \texttt{Linformer} linear-time self-attention function that plugs nicely into the \mingpt framework, replacing its original self-attention function in the Transformer.
%\item \textbf{Subwords instead of characters} Gosh, it must be a lot of work for my model to learn all of language, from the structure of words all the way to sentences and documents...
%In this component, you'll plug in an existing subword tokenization function from the Huggingface repository to replace the existing character-level tokenization of \mingpt. Because no one has time to implement tokenization themselves (at least \textit{you} don't, not if you're doing all the other stuff we're asking you to do.)
%\item \textbf{Running the model on stuff} Frequently you have to source and format your training data yourself, but this time we've got you. You'll \textit{pretrain} your encoder-decoder network on some relatively natural English text, and then \textit{finetune} that same network on a task that actually demonstrates some things that model learned at pretraining time.
%\end{enumerate}
\end{enumerate}
