{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f713a2fa",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'add_argument'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 188>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    189\u001b[0m     argp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcharcorruption\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m#argparse.ArgumentParser()\u001b[39;00m\n\u001b[1;32m--> 190\u001b[0m     \u001b[43margp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_argument\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset_type\u001b[39m\u001b[38;5;124m'\u001b[39m, help\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mType of dataset to sample from.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    191\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptions: namedata, charcorruption.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    192\u001b[0m             choices\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnamedata\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcharcorruption\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    193\u001b[0m     args \u001b[38;5;241m=\u001b[39m argp\u001b[38;5;241m.\u001b[39mparse_args()\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mdataset_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnamedata\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    196\u001b[0m         \u001b[38;5;66;03m# Even if it hasn't been implemented, we use it to define the vocab\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'add_argument'"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import argparse\n",
    "\n",
    "\"\"\"\n",
    "The input-output pairs (x, y) of the NameDataset are of the following form:\n",
    "\n",
    "  x: Where was Khatchig Mouradian born?⁇Lebanon⁇□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
    "  y: □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□⁇Lebanon⁇□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
    "  x: Where was Jacob Henry Studer born?⁇Columbus⁇□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
    "  y: □□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□⁇Columbus⁇□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
    "\n",
    "Using the PAD_CHAR characters in y before the ⁇[place] keeps the trainer from\n",
    "optimizing the model to predict the question, \"Where was...\".\n",
    "\n",
    "Note that the NameDataset should take the pretraining_dataset defined in run.py\n",
    "as an input. This is to allow the vocab specification of the NameDataset to be\n",
    "the same as that of the pretraining dataset.\n",
    "\n",
    "You don't need to implement anything in NameDataset.\n",
    "\"\"\"\n",
    "\n",
    "class NameDataset(Dataset):\n",
    "    def __init__(self, data, pretraining_dataset):\n",
    "        self.MASK_CHAR = u\"\\u2047\" # the doublequestionmark character, for mask\n",
    "        self.PAD_CHAR = u\"\\u25A1\" # the empty square character, for pad\n",
    "        self.itos = pretraining_dataset.itos \n",
    "        self.stoi = pretraining_dataset.stoi \n",
    "        self.block_size = pretraining_dataset.block_size\n",
    "        self.data = list(data.encode('utf-8').decode('ascii', errors='ignore').split('\\n'))\n",
    "\n",
    "    def __len__(self):\n",
    "        # returns the length of the dataset\n",
    "        return len(self.data) - 1\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inp, oup = self.data[idx].split('\\t')\n",
    "        x = inp + self.MASK_CHAR + oup + self.MASK_CHAR\n",
    "        x = x + self.PAD_CHAR*(self.block_size - len(x))\n",
    "        y = self.PAD_CHAR*(len(inp)-1) + x[len(inp):]\n",
    "        \n",
    "        x = x[:-1]\n",
    "        x = torch.tensor([self.stoi[c] for c in x], dtype=torch.long)\n",
    "        y = torch.tensor([self.stoi[c] for c in y], dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "[part e]\n",
    "\n",
    "Write a class that yields examples of a simplified span corruption objective.\n",
    "Do not change the signature of the __init__ or __getitem__ functions.\n",
    "\n",
    "Make sure to implement the full spec for full credit -- we list below the\n",
    "criteria that must be satisfied for a full implementation.\n",
    "\n",
    "--------------\n",
    "Vocabulary Specification\n",
    "\n",
    "Your vocabulary is to be accessible via two dictionaries:\n",
    "  self.stoi: a dictionary from characters in the vocabulary to indices of type\n",
    "      int\n",
    "  self.itos: a dictionary from indices of type int to characters in the\n",
    "      vocabulary\n",
    "\n",
    "Your vocabulary must have the following form: \n",
    "\n",
    "  Identifier 0 must be assigned to the unicode element u\"\\u25A1\".\n",
    "      This is the empty_square_character.\n",
    "      Further, let self.PAD_CHAR = u\"\\u25A1\"\n",
    "  Identifier 1 must be assigned to the unicode element u\"\\u2047\".\n",
    "      This is the doublequestionmark character, which we'll use\n",
    "      as a sentinel to represent that text is missing from the input\n",
    "      Further, let self.MASK_CHAR = u\"\\u2047\"\n",
    "  Identifiers 2, ..., len(self.itos)-1 should be the sorted list of characters\n",
    "      that appear in the data argument.\n",
    "\n",
    "--------------\n",
    "Masking Specification\n",
    "\n",
    "The __getitem__ function takes an index and returns a data point (x, y) where\n",
    "x and y are Long tensors of length self.block_size. x encodes the input\n",
    "sequence, and y encodes the output sequence.\n",
    "\n",
    "0. Use the idx argument of __getitem__ to retrieve the element of self.data\n",
    "at the given index. We'll call the resulting data entry a document.\n",
    "\n",
    "1. Randomly truncate the document to a length no less than 4 characters,\n",
    "and no more than int(self.block_size*7/8) characters.\n",
    "\n",
    "- IMPORTANT: You are free to decide how to perform this random truncation, but\n",
    "make sure that the length is picked _randomly_ (every possible length from 4\n",
    "to int(self.block_size*7/8) has a chance of being picked) for full credit.\n",
    "\n",
    "2. Now, break the (truncated) document into three substrings:\n",
    "    \n",
    "    [prefix] [masked_content] [suffix]\n",
    "\n",
    "  In other words, choose three strings prefix, masked_content and suffix\n",
    "    such that prefix + masked_content + suffix = [the original document].\n",
    "  The length of [masked_content] should be random, and 1/4 the length of the\n",
    "    truncated document on average.\n",
    "\n",
    "- IMPORTANT: You are free to decide how to perform this operation, but\n",
    "make sure that the length is picked _randomly_ (has a chance of being more or\n",
    "less than 1/4 the length of the truncated document) for full credit.\n",
    "\n",
    "3. Rearrange these substrings into the following form:\n",
    "\n",
    "    [prefix] MASK_CHAR [suffix] MASK_CHAR [masked_content] MASK_CHAR [pads]\n",
    "  \n",
    "  This resulting string, denoted masked_string, serves as the output example.\n",
    "  Here MASK_CHAR is the masking character defined in Vocabulary Specification,\n",
    "    and [pads] is a string of repeated PAD_CHAR characters chosen so that the\n",
    "    entire string is of length self.block_size.\n",
    "  Intuitively, the [masked_content], a string, is removed from the document and\n",
    "    replaced with MASK_CHAR (the masking character defined in Vocabulary\n",
    "    Specification). After the suffix of the string, the MASK_CHAR is seen again,\n",
    "    followed by the content that was removed, and the padding characters.\n",
    "\n",
    "4. We now use masked_string to construct the input and output example pair. To\n",
    "do so, simply take the input string to be masked_string[:-1], and the output\n",
    "string to be masked_string[1:]. In other words, for each character, the goal is\n",
    "to predict the next character in the masked string.\n",
    "\n",
    "5. Making use of the vocabulary that you defined, encode the resulting input\n",
    "and output strings as Long tensors and return the resulting data point.\n",
    "\n",
    "----------------\n",
    "Here are some examples of input-output pairs (x, y):\n",
    "\n",
    "  x: Khatchig Mouradian. Khatchig Mouradian is a jour⁇and tran⁇nalist, writer ⁇□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
    "  y: hatchig Mouradian. Khatchig Mouradian is a jour⁇and tran⁇nalist, writer ⁇□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
    "\n",
    "  x: Jaco⁇enry ⁇b H⁇□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
    "  y: aco⁇enry ⁇b H⁇□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
    "\n",
    "  x: John Stephen. Born in Glasgow, Steph⁇lder's apprentice on⁇en became a we⁇□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
    "  y: ohn Stephen. Born in Glasgow, Steph⁇lder's apprentice on⁇en became a we⁇□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□□\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "class CharCorruptionDataset(Dataset):\n",
    "    def __init__(self, data, block_size):\n",
    "        self.MASK_CHAR = u\"\\u2047\" # the doublequestionmark character, for mask\n",
    "        self.PAD_CHAR = u\"\\u25A1\" # the empty square character, for pad\n",
    "\n",
    "        chars = list(sorted(list(set(data))))\n",
    "        assert self.MASK_CHAR not in chars \n",
    "        assert self.PAD_CHAR not in chars\n",
    "        chars.insert(0, self.MASK_CHAR)\n",
    "        chars.insert(0, self.PAD_CHAR)\n",
    "\n",
    "        self.stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "        self.itos = { i:ch for i,ch in enumerate(chars) }\n",
    "\n",
    "        data_size, vocab_size = len(data), len(chars)\n",
    "        print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
    "\n",
    "        self.block_size = block_size\n",
    "        self.max_context_size = int(block_size*3/4)\n",
    "        self.masking_percent = 0.25\n",
    "        self.vocab_size = vocab_size\n",
    "        self.data = data.split('\\n')\n",
    "        self.item = None\n",
    "\n",
    "    def __len__(self):\n",
    "        # returns the length of the dataset\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        ### TODO:\n",
    "        ### [part e]: see spec above\n",
    "\n",
    "        ### START CODE HERE\n",
    "        document =self.data[idx]\n",
    "        print(document)\n",
    "        ### END CODE HERE\n",
    "\n",
    "        raise NotImplementedError\n",
    "\n",
    "\"\"\"\n",
    "Code under here is strictly for your debugging purposes; feel free to modify\n",
    "as desired.\n",
    "\"\"\"\n",
    "if __name__ == '__main__':\n",
    "    argp = str('charcorruption') #argparse.ArgumentParser()\n",
    "    argp.add_argument('dataset_type', help=\"Type of dataset to sample from.\"\n",
    "            \"Options: namedata, charcorruption.\",\n",
    "            choices=[\"namedata\", \"charcorruption\"])\n",
    "    args = argp.parse_args()\n",
    "\n",
    "    if args.dataset_type == 'namedata':\n",
    "        # Even if it hasn't been implemented, we use it to define the vocab\n",
    "        corruption_dataset = CharCorruptionDataset(open('./../data/wiki.txt', encoding='utf-8').read(), 128) \n",
    "        # Make the name dataset\n",
    "        name_dataset = NameDataset(open('./../data/birth_places_train.tsv', encoding='utf-8').read(),\n",
    "                corruption_dataset)\n",
    "        for _, example in zip(range(4), name_dataset):\n",
    "            x, y = example\n",
    "            print('x:', ''.join([name_dataset.itos[int(c)] for c in x]))\n",
    "            print('y:', ''.join([name_dataset.itos[int(c)] for c in y]))\n",
    "        pass\n",
    "    elif args.dataset_type == 'charcorruption':\n",
    "        corruption_dataset = CharCorruptionDataset(open('./../data/wiki.txt', encoding='utf-8').read(), 128) \n",
    "        for _, example in zip(range(4), corruption_dataset):\n",
    "            x, y = example\n",
    "            print('x:', ''.join([corruption_dataset.itos[int(c)] for c in x]))\n",
    "            print('y:', ''.join([corruption_dataset.itos[int(c)] for c in y]))\n",
    "    else:\n",
    "        raise ValueError(\"Unknown dataset type in command line args: {}\"\n",
    "                .format(args.dataset_type))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85603020",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
